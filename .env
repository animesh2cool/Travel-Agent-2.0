# .env file configuration
# Create this file in your project root directory

# Ollama Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=ollama/llama3.2:latest

# Weather API Configuration (OpenWeatherMap - Free tier available)
# Get your free API key from: https://openweathermap.org/api
OPENROUTER_API_KEY=sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
WEATHER_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxx
PREDICTHQ_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxx
SERPAPI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Optional: Logging level
LOG_LEVEL=INFO

# ============================================================================
# SETUP INSTRUCTIONS
# ============================================================================

# 1. Install Ollama (if you haven't already)
# Visit: https://ollama.com/download
# Or use: curl -fsSL https://ollama.com/install.sh | sh

# 2. Pull Llama 3.2 model
# ollama pull llama3.2

# 3. Start Ollama service (if not running)
# ollama serve

# 4. Install Python dependencies
# pip install fastapi uvicorn crewai httpx python-dotenv requests

# 5. Get free weather API key
# - Go to: https://openweathermap.org/api
# - Sign up for free account
# - Get your API key from dashboard
# - Add it to the .env file above

# 6. Create project structure:
# project/
# ├── main.py (your main application file)
# ├── .env (this configuration file)
# ├── public/ (optional - for static frontend files)
# └── requirements.txt

# 7. Run the application
# python main.py
# or
# uvicorn main:app --reload --host 0.0.0.0 --port 8000

# 8. Test the application
# Visit: http://localhost:8000/health
# API endpoint: http://localhost:8000/plan
